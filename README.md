# ğŸ“¸ OfflineCaptioning â€” On-Device Image Captioning for Android

> Generate natural language descriptions of images **entirely on your Android device** â€” no internet, no cloud, no privacy concerns.

![Android](https://img.shields.io/badge/Android-3DDC84?style=for-the-badge&logo=android&logoColor=white)
![Kotlin](https://img.shields.io/badge/Kotlin-7F52FF?style=for-the-badge&logo=kotlin&logoColor=white)
![ONNX](https://img.shields.io/badge/ONNX-005CED?style=for-the-badge&logo=onnx&logoColor=white)

---

## ğŸ¯ What is this?

**OfflineCaptioning** is an Android app that uses a Vision-Language AI model (BLIP) to automatically describe what it sees through your camera â€” all processed locally on your device.

Point your camera at anything. Get a caption instantly. No data ever leaves your phone.

> ğŸ“· *"a laptop computer sitting on top of a desk"*
> ğŸ“· *"a person sitting on a chair near a window"*

---

## âœ¨ Features

- ğŸ”’ **Fully offline** â€” works without any internet connection
- âš¡ **Real-time** â€” caption generated on camera button press
- ğŸ§  **BLIP-base model** â€” state-of-the-art vision-language AI
- ğŸ“± **On-device inference** â€” powered by ONNX Runtime
- ğŸ” **Privacy-first** â€” no images are sent to any server

---

## ğŸ—ï¸ How It Works

```
ğŸ“· Camera (CameraX)
        â†“
ğŸ–¼ï¸  Image Preprocessing
        â†“
ğŸ‘ï¸  Vision Encoder  â”€â”€â†’  Image Features
        â†“
ğŸ“  Text Decoder    â”€â”€â†’  Token Generation (Greedy Decoding)
        â†“
ğŸ’¬  Caption Output on Screen
```

---

## ğŸ“¦ Tech Stack

| Layer | Technology |
|-------|-----------|
| Language | Kotlin |
| Camera | CameraX API |
| AI Model | BLIP-base (Salesforce) |
| Model Format | ONNX |
| Inference Engine | ONNX Runtime for Android v1.20.0 |
| Min SDK | Android 8.0 (API 26) |

---

## âš™ï¸ Installation

### Prerequisites
- Android Studio (latest)
- Android device with Android 8.0+
- ADB installed
- ~1 GB free storage on device
- Python 3.8+ (for model export only)

---

### Step 1 â€” Clone the repository

```bash
git clone https://github.com/mahamudul-hasan-cse/offline-image-captioning.git
cd offline-image-captioning
```

---

### Step 2 â€” Export the BLIP model (one-time setup)

The model files are not included in this repo due to their size (~900 MB). Export them once using the provided script.

**Install dependencies:**
```bash
pip install torch transformers onnx onnxruntime optimum
```

**Run the export script:**
```bash
python scripts/export_model.py
```

This will generate the following files inside `models/` folder:
```
models/
â”œâ”€â”€ blip_vision_encoder.onnx
â”œâ”€â”€ blip_vision_encoder.onnx.data
â”œâ”€â”€ blip_text_decoder.onnx
â”œâ”€â”€ blip_text_decoder.onnx.data
â”œâ”€â”€ tokenizer.json
â””â”€â”€ vocab.txt
```

> â±ï¸ Export takes around 5â€“10 minutes depending on your machine.

---

### Step 3 â€” Push models to your Android device

Connect your device via USB with USB Debugging enabled, then run:

```bash
# Create the models directory on device
adb shell mkdir -p /sdcard/Android/data/com.example.offlinecaptioning/files/models

# Push all model files
adb push models/blip_vision_encoder.onnx /sdcard/Android/data/com.example.offlinecaptioning/files/models/
adb push models/blip_vision_encoder.onnx.data /sdcard/Android/data/com.example.offlinecaptioning/files/models/
adb push models/blip_text_decoder.onnx /sdcard/Android/data/com.example.offlinecaptioning/files/models/
adb push models/blip_text_decoder.onnx.data /sdcard/Android/data/com.example.offlinecaptioning/files/models/
adb push models/tokenizer.json /sdcard/Android/data/com.example.offlinecaptioning/files/models/
adb push models/vocab.txt /sdcard/Android/data/com.example.offlinecaptioning/files/models/
```

---

### Step 4 â€” Build and run

1. Open the project in **Android Studio**
2. Click **Sync Project with Gradle Files**
3. Select your device and click **â–¶ Run**

---

## ğŸ“ Project Structure

```
OfflineCaptioning/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ src/main/
â”‚       â”œâ”€â”€ java/com/example/offlinecaptioning/
â”‚       â”‚   â”œâ”€â”€ MainActivity.kt          # Camera UI & entry point
â”‚       â”‚   â””â”€â”€ CaptioningViewModel.kt   # BLIP inference pipeline
â”‚       â”œâ”€â”€ res/                         # Layouts and resources
â”‚       â””â”€â”€ AndroidManifest.xml
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ export_model.py                  # BLIP â†’ ONNX export script
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ“Š Performance

| Metric | Result |
|--------|--------|
| Model size (total) | ~900 MB (float32) |
| Quantized size | ~224 MB (INT8, planned) |
| Inference | Fully on-device |
| Internet required | âŒ None |

---

## ğŸ—ºï¸ Roadmap

- [x] BLIP-base export to ONNX format
- [x] CameraX integration
- [x] On-device inference pipeline
- [x] Working offline caption generation
- [ ] INT8 quantization (target: ~224 MB)
- [ ] Inference latency benchmarking
- [ ] Support for multiple languages

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to open issues or submit pull requests.

---

## ğŸ‘¤ Author

**Md. Mahamudul Hasan**
GitHub: [@mahamudul-hasan-cse](https://github.com/mahamudul-hasan-cse)

---

## ğŸ“„ License

MIT License â€” feel free to use, modify, and distribute.
